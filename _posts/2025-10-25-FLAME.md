---
title: "FLAME: Frozen Large Language Models Enable Data-Efficient Language-Image Pre-training"
author: seokgi
date: 2025-10-25
categories: [Paper]
tags: [LLM, text-image retrieval, dual encoder]
pin: true
math: true
---
Written By. Anjia Cao, Xing Wei, Zhiheng Ma

- FLAME(Frozen Large Language Models Enable data-efficient language-image pre-training)은 **Frozen LLM**을 텍스트 인코더로 활용하여 기존 CLIP 모델의 한계를 극복하는 새로운 프레임워크

## 1. 기존 비전-언어 사전 학습의 한계점

![image.png](https://seokilee0412.github.io/assets/img/FLAME/image.png)

1. **제한된 훈련 데이터 쌍**: 특히 장문 설명이나 비영어권 데이터 확보가 어려움.
2. **제한된 모델 용량 (77 토큰 길이 제한)**: 표준 CLIP 텍스트 인코더는 최대 77 토큰 길이 제한으로 인해 풍부한 장문 컨텍스트 데이터를 활용하기 어려움.
3. **기존 해결책의 한계**:
    - 데이터 증강(합성 캡션 생성, 번역)은 텍스트 인코더의 용량 병목 현상으로 인해 비효율적.
    - Positional Encoding Interpolation 같은 모델 중심 접근법은 CLIP 설계 자체의 제약(최대 77 토큰)을 근본적으로 해결하지 못함.

## 2. FLAME 프레임워크

![image.png](https://seokilee0412.github.io/assets/img/FLAME/image1.png)

FLAME은 **Frozen LLM**을 텍스트 인코더로 사용하여 데이터 효율적인 언어-이미지 사전 학습을 가능하게 함.

### **Frozen LLM**

- 장문 캡션을 직접 처리하며, 사전 학습된 LLM의 다국어 및 장문 처리 능력을 유지.

### **Multifaceted Prompt Distillation**

- 제한된 훈련 데이터의 유용성을 극대화하기 위해 LLM에서 다양한 의미적 측면을 추출하도록 유도.
- LLM의 디코더 전용 구조(인과적 어텐션) 한계를 극복하고 이미지의 다면적 의미를 포착하기 위해 도입.
1. **Hierarchical Semantic Decomposition**: 인간의 시각 인식을 모방하여 시각적 의미를 세 가지 수준으로 분해.
    - **Entity Level**: 개체 및 속성 포착 (미세 조정 시각 구별).
    - **Interaction Level**: 행동 및 이벤트 포착 (장면의 동적/관계적 측면 이해).
    - **Scene Level**: 분위기, 감정 등 추상적 개념 포착 (전체적 맥락).
2. **프롬프트 엔지니어링 원칙**:
    - **Semantic Distinctiveness**: 각 프롬프트가 추출된 표현의 중복을 최소화하도록 서로 다른 시각적 의미를 목표.
    - **Constrained Output Space**: LLM이 복잡한 시각 개념을 간결하고 구별되는 특징으로 증류하도록 **단일 단어 응답**을 생성하도록 강제.
    - **Cognitive Alignment**: 프롬프트 구조가 인간의 시각 처리를 모방하여 추상화 수준을 명시적으로 분리.
- 쿼리와 관련된 여러 시맨틱 관점의 벡터들이 생기므로,
- 사용자는 단순한 "텍스트 vs 이미지" 정렬보다 더 **정교한 시맨틱 매칭이 가능.**
- **FLAMEMultifaceted Prompt Distillation 입력 텍스트를 다르게 "질문해보는 방식으로 의미를 분해"하는 것**이지, 쿼리와 무관한 질문을 생성하는 것이 아님.

### **Facet-Decoupled Attention**

![image.png](https://seokilee0412.github.io/assets/img/FLAME/image2.png)

- 여러 의미적 측면의 독립성을 유지하면서 단일 패스 추론을 가능하게 하여 효율적인 계산을 보장.
- 다각적 접근법의 계산 오버헤드를 줄이기 위해 단일 패스 추론을 가능하게 하는 어텐션 메커니즘.
- **Facet-Decoupled Attention Mask (FDA) 방법**
    - 입력 시퀀스 구조 → 한 번에 여러 프롬프트를 넣음
        
        ```
        "Image description: {캡션}. After thinking step by step, "
        "Prompt 1?"
        "Prompt 2?"
        ...
        "Prompt K?"
        ```
        
    - Attention Mask 설계
        - **Prefix 영역 (image caption)**: 모든 프롬프트가 이 부분은 **공유해서 볼 수 있음**.
        - **각 프롬프트의 응답 위치**: **해당 프롬프트 영역 외에는 접근 불가**하도록 마스킹.
    - 효율 최적화
        - 공유 prefix에 대한 **KV 캐시(Cache)를 한 번만 계산**해서 모든 prompt에 재사용.
    - 결과적으로, **한 번의 LLM 호출로 K개의 독립적인 시맨틱 임베딩을 추출 가능**.
- 추론 시 여러 프롬프트에 대해 나온 임베딩을 어떻게 텍스트 글로벌 임베딩으로 바꾸는지
    - 개별 facet들에 대한 **이미지 임베딩과의** 유사도를 계산 후 평균

### **Offline 임베딩 전략**

- 학습 전에 텍스트 임베딩을 미리 생성하여 훈련 속도 개선.

### **Trainable Projection Layer 활용**

- 텍스트 및 이미지 임베딩을 공유 의미 공간에 정렬하기 위한 프로젝션 레이어

---

## 실험 결과 및 성능 평가

- 시각 인코더는 ViT-B/16, LLM은 Mistral-Nemo를 기본으로 사용.
1. **데이터셋**
    - CC3M: Conceptual Captions 3 Million의 약자로, 3백만 개의 이미지-캡션 쌍
    - YFCC15M: Yahoo Flickr Creative Commons 15 Million의 약자로, 1,500만 개의 이미지-텍스트 쌍.
    - WIT-400M: Web Image Text 400M의 약자로, 4억 개의 이미지-텍스트 쌍.

### **Long-Context Retrieval**

![image.png](https://seokilee0412.github.io/assets/img/FLAME/image3.png)

- Urban-1k 텍스트-이미지 검색에서 WIT-400M 훈련 CLIP 대비 **34.6% 향상**된 87.9%의 Recall@1 달성.
- S4V-eval에서 평균 14.8% 향상.
    - S4V: ShareGPT4V의 약자로, 특히 장문 캡션을 포함하는 데이터셋

### **Short-Context Retrieval**

![image.png](https://seokilee0412.github.io/assets/img/FLAME/image4.png)

- Short에서도 좋은 성능

### **Multilingual Zero-Shot Retrieval**

![image.png](https://seokilee0412.github.io/assets/img/FLAME/image5.png)

- Crossmodal-3600 (36개 언어)에서 WIT-400M 훈련 CLIP 대비 평균 이미지-텍스트 Recall@1에서 **44.4% 향상** 달성.

### **Zero-Shot Image Classification**

![image.png](https://seokilee0412.github.io/assets/img/FLAME/image6.png)

- CC3M 훈련 시 ImageNet Top-1 정확도에서 기존 SOTA 대비 **4.9% 향상**.
- 10개 다운스트림 데이터셋에서 평균 정확도 40.3% 달성 (이전 SOTA 대비 9.3% 향상).

### **Ablation on Number of Prompts**

![image.png](https://seokilee0412.github.io/assets/img/FLAME/image7.png)

- 프롬프트가 너무 적으면 장문 벤치마크에서 성능이 저하되나, 7개 프롬프트 사용 시 균형 잡힌 최적 성능 달성.

---

## 결론

- FLAME은 LLM을 “고정” 상태에서 효과적으로 활용할 수 있음을 실증한 **프레임워크.**
    - **데이터 효율성 강화**: 적은 양의 데이터로도 높은 성능 달성
    - **멀티모달 학습의 새로운 접근**: 프롬프트 기반의 시맨틱 분해
    - **범용성 확보**: 다국어, 장문, 일반화에서 우수한 결과